{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53ed2431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
    "# Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "# XGBoost\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "# LightGBM\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "# CatBoost\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e47f52b",
   "metadata": {},
   "source": [
    "# Regression Comparison of All Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b20498d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Regressor: MSE = 1027.7920662466356 R2 = 0.9665940401150309\n",
      "XGBoost Regressor: MSE = 1214.0679278931755 R2 = 0.9605395820528794\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000406 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score -5.915670\n",
      "LightGBM Regressor: MSE = 1715.8620906361114 R2 = 0.944229944898043\n",
      "CatBoost Regressor: MSE = 490.5659406264835 R2 = 0.9840553097540958\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Dataset\n",
    "X_reg, y_reg = make_regression(n_samples=2000, n_features=20, noise=0.2, random_state=42)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gbr.fit(X_train_reg, y_train_reg)\n",
    "y_pred_gbr = gbr.predict(X_test_reg)\n",
    "print(\"Gradient Boosting Regressor: MSE =\", mean_squared_error(y_test_reg, y_pred_gbr), \"R2 =\", r2_score(y_test_reg, y_pred_gbr))\n",
    "\n",
    "# XGBoost Regressor\n",
    "xgb = XGBRegressor(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "xgb.fit(X_train_reg, y_train_reg)\n",
    "y_pred_xgb = xgb.predict(X_test_reg)\n",
    "print(\"XGBoost Regressor: MSE =\", mean_squared_error(y_test_reg, y_pred_xgb), \"R2 =\", r2_score(y_test_reg, y_pred_xgb))\n",
    "\n",
    "# LightGBM Regressor\n",
    "lgbm = LGBMRegressor(n_estimators=200, learning_rate=0.1, max_depth=-1, random_state=42)\n",
    "lgbm.fit(X_train_reg, y_train_reg)\n",
    "y_pred_lgbm = lgbm.predict(X_test_reg)\n",
    "print(\"LightGBM Regressor: MSE =\", mean_squared_error(y_test_reg, y_pred_lgbm), \"R2 =\", r2_score(y_test_reg, y_pred_lgbm))\n",
    "\n",
    "# CatBoost Regressor\n",
    "cat = CatBoostRegressor(n_estimators=200, learning_rate=0.1, depth=6, random_state=42, verbose=0)\n",
    "cat.fit(X_train_reg, y_train_reg)\n",
    "y_pred_cat = cat.predict(X_test_reg)\n",
    "print(\"CatBoost Regressor: MSE =\", mean_squared_error(y_test_reg, y_pred_cat), \"R2 =\", r2_score(y_test_reg, y_pred_cat))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f8bf0f",
   "metadata": {},
   "source": [
    "# Classification Comparison of All Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d35c114a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classifier: Accuracy = 0.8475\n",
      "XGBoost Classifier: Accuracy = 0.845\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000516 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5100\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score -1.088043\n",
      "[LightGBM] [Info] Start training from score -1.102997\n",
      "[LightGBM] [Info] Start training from score -1.104882\n",
      "LightGBM Classifier: Accuracy = 0.855\n",
      "CatBoost Classifier: Accuracy = 0.8375\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "X_clf, y_clf = make_classification(n_samples=2000, n_features=20, n_classes=3, n_informative=10, random_state=42)\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    X_clf, y_clf, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "gbc = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gbc.fit(X_train_clf, y_train_clf)\n",
    "y_pred_gbc = gbc.predict(X_test_clf)\n",
    "print(\"Gradient Boosting Classifier: Accuracy =\", accuracy_score(y_test_clf, y_pred_gbc))\n",
    "\n",
    "# XGBoost Classifier\n",
    "xgb_c = XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb_c.fit(X_train_clf, y_train_clf)\n",
    "y_pred_xgb_c = xgb_c.predict(X_test_clf)\n",
    "print(\"XGBoost Classifier: Accuracy =\", accuracy_score(y_test_clf, y_pred_xgb_c))\n",
    "\n",
    "# LightGBM Classifier\n",
    "lgbm_c = LGBMClassifier(n_estimators=200, learning_rate=0.1, max_depth=-1, random_state=42)\n",
    "lgbm_c.fit(X_train_clf, y_train_clf)\n",
    "y_pred_lgbm_c = lgbm_c.predict(X_test_clf)\n",
    "print(\"LightGBM Classifier: Accuracy =\", accuracy_score(y_test_clf, y_pred_lgbm_c))\n",
    "\n",
    "# CatBoost Classifier\n",
    "cat_c = CatBoostClassifier(n_estimators=200, learning_rate=0.1, depth=6, random_state=42, verbose=0)\n",
    "cat_c.fit(X_train_clf, y_train_clf)\n",
    "y_pred_cat_c = cat_c.predict(X_test_clf)\n",
    "print(\"CatBoost Classifier: Accuracy =\", accuracy_score(y_test_clf, y_pred_cat_c))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
